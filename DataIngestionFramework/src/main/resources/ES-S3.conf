movoto {
 analytics {
   etl {
    appName = "Movoto Data Ingestion Framework"
    migrationType = "ES-S3" # Other values are HDFS-S3, S3-S3, HIVE-HIVE, HDFS-HDFS, S3-HIVE, S3-HDFS, RDBMS-S3, RDBMS-HDFS, RDBMS-HIVE, S3-RDBMS, HIVE-RDBMS, HDFS-RDBMS, ES-ES, ES-S3, S3-ES
    source {
     type = "ES"
     input = "mls_stats/_doc"
     format = "csv"     # Use this only while reading from File storage. Default is parquet
     inputOptionKeys = ["pushdown","es.nodes","es.port"]
     inputOptionValues = ["true","10.77.7.156","9200"]
    }
    target {
     type = "S3"
     ingestionComplexity = "SMALL"    # Default is small. Other possible values are medium and large Case small: no partition columns, case medium: 1 partition per partition column, case large: n partitions per partition column
     numPartitions = 1          # Default number of partitions is 1. If required, pass the numeric value. Ex: 100, 200 etc.,
     modeType = "overwrite"     # Default mode type is append. If required, pass the mode accordingly. Ex: "overwrite"
     partitionColumns = []      # Default partition columns are empty list of string type. If required, pass the list of values. Ex: ["year","month","day"]
     format = "csv"         # Default parquet. Possible values are csv,json,orc, avro etc.,
     outputOptionKeys = []     # For text files like csv and json, more option elements can be passed. Examples: sep, header, nullValue etc., Refer Spark API docs for more details
     outputOptionValues = []   # For text files like csv and json, kindly use bzip2 compression. For parquet, use snappy. For others, do some research or contact me
     outputPath = "s3a://bigdata.analytics.movoto.com/test/mls_stats/"
    }
    sparkConf = [
     "spark.driver.cores = 1"
     "spark.driver.memory = 2g"
     "spark.executor.instances = 10"
     "spark.executor.cores = 5"
     "spark.executor.memory = 6g"
     "spark.logConf = true"
     "spark.serializer = org.apache.spark.serializer.KryoSerializer"]
     }
 }
}