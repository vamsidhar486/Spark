movoto {
 analytics {
   etl {
    appName = "Movoto Data Ingestion Framework"
    migrationType = "HIVE-S3" # Other values are HDFS-S3, S3-S3, HIVE-HIVE, HDFS-HDFS, S3-HIVE, S3-HDFS, RDBMS-S3, RDBMS-HDFS, RDBMS-HIVE, S3-RDBMS, HIVE-RDBMS, HDFS-RDBMS, ES-ES, ES-S3, S3-ES
    source {
     type = "Hive"
     input = """SELECT zipcode,
                   city_name,
                   county_name,
                   ast_region,
                   state,
                   dma_name,
                   dma_code,
                   tier,
                   igen_county_code,
                   center_lat,
                   center_lng,
                   timezone,
                   pst_offset
              FROM mls.zipcode_usps"""
     format = "csv"     # Use this only while reading from File storage. Default is parquet
    }
    target {
     type = "S3"
     ingestionComplexity = "SMALL"    # Default is small. Other possible values are medium and large Case small: no partition columns, case medium: 1 partition per partition column, case large: n partitions per partition column
     numPartitions = 1          # Default number of partitions is 1. If required, pass the numeric value. Ex: 100, 200 etc.,
     modeType = "overwrite"     # Default mode type is append. If required, pass the mode accordingly. Ex: "overwrite"
     partitionColumns = []      # Default partition columns are empty list of string type. If required, pass the list of values. Ex: ["year","month","day"]
     format = "parquet"         # Default parquet. Possible values are csv,json,orc, avro etc.,
     outputOptionKeys = ["compression"]     # For text files like csv and json, more option elements can be passed. Examples: sep, header, nullValue etc., Refer Spark API docs for more details
     outputOptionValues = ["snappy"]        # For text files like csv and json, kindly use bzip2 compression. For parquet, use snappy. For others, do some research or contact me
     outputPath = "s3a://bigdata.analytics.movoto.com/user/aws/warehouse/mls.db/zipcode_usps/"
    }
    sparkConf = [
     "spark.driver.cores = 1"
     "spark.driver.memory = 2g"
     "spark.executor.instances = 10"
     "spark.executor.cores = 5"
     "spark.executor.memory = 6g"
     "spark.logConf = true"
     "spark.serializer = org.apache.spark.serializer.KryoSerializer"]
     }
 }
}